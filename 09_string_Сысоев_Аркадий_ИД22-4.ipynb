{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 311 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=8f875b483616d820edd6c6995d4469f291a7b7cd327fce0a6b9e4cd324046759\n",
      "  Stored in directory: /Users/sysoevkd/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /Users/sysoevkd/opt/anaconda3/lib/python3.9/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: Levenshtein==0.21.0 in /Users/sysoevkd/opt/anaconda3/lib/python3.9/site-packages (from python-Levenshtein) (0.21.0)\r\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /Users/sysoevkd/opt/anaconda3/lib/python3.9/site-packages (from Levenshtein==0.21.0->python-Levenshtein) (3.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "with open('litw-win.txt', 'r', encoding = 'cp1251') as f:\n",
    "    words = [line.split()[-1] for line in f]\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance as lev\n",
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance(text, words)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "import re\n",
    "snb_stemmer_ru = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "счита\n",
      "слов\n",
      "из\n",
      "файл\n",
      "litw-win.txt\n",
      "и\n",
      "запиш\n",
      "их\n",
      "в\n",
      "список\n",
      "words.\n",
      "в\n",
      "зада\n",
      "предложен\n",
      "исправьт\n",
      "все\n",
      "опечатки,\n",
      "замен\n",
      "слов\n",
      "с\n",
      "опечатк\n",
      "на\n",
      "ближайш\n",
      "(в\n",
      "смысл\n",
      "расстоян\n",
      "левенштейна)\n",
      "к\n",
      "ним\n",
      "слов\n",
      "из\n",
      "списк\n",
      "words.\n",
      "считайте,\n",
      "что\n",
      "в\n",
      "слов\n",
      "ест\n",
      "опечатка,\n",
      "есл\n",
      "дан\n",
      "слов\n",
      "не\n",
      "содерж\n",
      "в\n",
      "списк\n",
      "words.\n"
     ]
    }
   ],
   "source": [
    "import nltk   \n",
    "ps = SnowballStemmer('russian')\n",
    "for i in text:\n",
    "    print(ps.stem(i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sysoevkd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/sysoevkd/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['litw',\n",
       " 'txt',\n",
       " 'win',\n",
       " 'words',\n",
       " 'ближайшие',\n",
       " 'все',\n",
       " 'данное',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'заданном',\n",
       " 'заменив',\n",
       " 'запишите',\n",
       " 'из',\n",
       " 'исправьте',\n",
       " 'их',\n",
       " 'левенштейна',\n",
       " 'на',\n",
       " 'не',\n",
       " 'ним',\n",
       " 'опечатка',\n",
       " 'опечатками',\n",
       " 'опечатки',\n",
       " 'предложении',\n",
       " 'расстояния',\n",
       " 'слова',\n",
       " 'слове',\n",
       " 'слово',\n",
       " 'смысле',\n",
       " 'содержится',\n",
       " 'списка',\n",
       " 'списке',\n",
       " 'список',\n",
       " 'считайте',\n",
       " 'файла',\n",
       " 'что']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'.split()\n",
    "cv = CountVectorizer()\n",
    "# векторизуем корпус:\n",
    "corpus_cv = cv.fit_transform(text)\n",
    "corpus_cv\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ar = corpus_cv.toarray()\n",
    "cv_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sysoevkd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['litw',\n",
       " 'txt',\n",
       " 'win',\n",
       " 'words',\n",
       " 'ближайшие',\n",
       " 'все',\n",
       " 'данное',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'заданном',\n",
       " 'заменив',\n",
       " 'запишите',\n",
       " 'из',\n",
       " 'исправьте',\n",
       " 'их',\n",
       " 'левенштейна',\n",
       " 'на',\n",
       " 'не',\n",
       " 'ним',\n",
       " 'опечатка',\n",
       " 'опечатками',\n",
       " 'опечатки',\n",
       " 'предложении',\n",
       " 'расстояния',\n",
       " 'слова',\n",
       " 'слове',\n",
       " 'слово',\n",
       " 'смысле',\n",
       " 'содержится',\n",
       " 'списка',\n",
       " 'списке',\n",
       " 'список',\n",
       " 'считайте',\n",
       " 'файла',\n",
       " 'что']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'.split()\n",
    "cv = CountVectorizer()\n",
    "# векторизуем корпус:\n",
    "corpus_cv = cv.fit_transform(text)\n",
    "corpus_cv\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sysoevkd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      name  \\\n",
       "0           0     george s at the cove  black bean soup   \n",
       "1           1        healthy for them  yogurt popsicles   \n",
       "2           2              i can t believe it s spinach   \n",
       "3           3                      italian  gut busters   \n",
       "4           4  love is in the air  beef fondue   sauces   \n",
       "\n",
       "                           preprocessed_descriptions  \n",
       "0  an original recipe created by chef scott meska...  \n",
       "1  my children and their friends ask for my homem...  \n",
       "2              these were so go it surprised even me  \n",
       "3  my sisterinlaw made these for us at a family g...  \n",
       "4  i think a fondue is a very romantic casual din...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_descriptions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'equally',\n",
       " 'dreaming',\n",
       " 'factor',\n",
       " 'adviehbaharat',\n",
       " 'otherwisebam',\n",
       " 'grouprecipescomat',\n",
       " 'heaping',\n",
       " 'mittens',\n",
       " 'polka',\n",
       " 'treatsthere',\n",
       " 'ethel',\n",
       " 'seaweed',\n",
       " 'messiah',\n",
       " '26gmono',\n",
       " 'nofrills',\n",
       " 'honeydinner',\n",
       " 'jeffersonday',\n",
       " 'wanting',\n",
       " 'appetisers',\n",
       " 'granule',\n",
       " 'stoneware',\n",
       " 'httpwwwinsanitytheorynet',\n",
       " 'wool',\n",
       " 'marcella',\n",
       " 'mochiko',\n",
       " 'mccormickmild',\n",
       " 'insurance',\n",
       " 'flatten',\n",
       " 'chocolatepeanut',\n",
       " 'lowi',\n",
       " 'perfected',\n",
       " 'bakinga',\n",
       " 'ks',\n",
       " 'balloons',\n",
       " 'barbecuer',\n",
       " 'hummusshe',\n",
       " 'custards',\n",
       " 'drips',\n",
       " 'slowcooker',\n",
       " 'marigolds',\n",
       " 'timeintensive',\n",
       " 'secondguess',\n",
       " '06',\n",
       " 'daydhs',\n",
       " 'enjoyprep',\n",
       " 'valastros',\n",
       " 'mayothis',\n",
       " 'fermenting',\n",
       " 'blueor',\n",
       " 'craisin',\n",
       " 'adornment',\n",
       " '1890s',\n",
       " 'chemist',\n",
       " 'angus',\n",
       " 'toxins',\n",
       " 'overdone',\n",
       " 'aftersiesta',\n",
       " 'textured',\n",
       " 'chock',\n",
       " 'luxuriously',\n",
       " 'weep',\n",
       " 'remove',\n",
       " '424623',\n",
       " 'nuking',\n",
       " 'specifies',\n",
       " '69591',\n",
       " '243518',\n",
       " 'rises',\n",
       " 'laboring',\n",
       " 'healing',\n",
       " 'eggandpotato',\n",
       " 'madaline',\n",
       " 'henry',\n",
       " 'detail',\n",
       " 'steakhouse',\n",
       " 'groom',\n",
       " 'humor',\n",
       " '091608',\n",
       " 'dishfor',\n",
       " 'sweetenfancy',\n",
       " 'gailanng',\n",
       " 'aheadneeds',\n",
       " 'encore',\n",
       " 'pillars',\n",
       " 'crescents',\n",
       " 'totmato',\n",
       " 'rajmah',\n",
       " 'ba',\n",
       " 'chronicle',\n",
       " 'counter',\n",
       " '72',\n",
       " 'grana',\n",
       " 'eleanor',\n",
       " 'mist',\n",
       " 'amalgamation',\n",
       " 'flax4life',\n",
       " 'sects',\n",
       " 'entresize',\n",
       " 'sb',\n",
       " 'coming',\n",
       " 'muchim',\n",
       " 'isthe',\n",
       " 'kombu',\n",
       " 'pimms',\n",
       " 'detroit',\n",
       " 'huei',\n",
       " 'doughsource',\n",
       " 'myhave',\n",
       " 'theclothesmakethegirlblogspotcom',\n",
       " 'kabobsyou',\n",
       " 'deepen',\n",
       " 'rawfoodrightnowblogspotcom',\n",
       " 'usin',\n",
       " 'diego',\n",
       " 'listened',\n",
       " 'tested',\n",
       " 'mayonnise',\n",
       " 'httpwwwveganlunchboxcomloafstudiohtml',\n",
       " 'cookingthat',\n",
       " 'steel',\n",
       " 'recipesavory',\n",
       " 'friend',\n",
       " 'iit',\n",
       " 'sandy',\n",
       " 'samples',\n",
       " 'jasmine',\n",
       " 'tipple',\n",
       " 'coauld',\n",
       " '55514',\n",
       " 'soupwhen',\n",
       " 'brainfreeze',\n",
       " 'priscilla',\n",
       " 'dora',\n",
       " 'obrien',\n",
       " 'petalstunisians',\n",
       " 'annapolis',\n",
       " '129211',\n",
       " 'ghee',\n",
       " 'tummywarmer',\n",
       " 'etc',\n",
       " 'substantial',\n",
       " '61906poor',\n",
       " 'sack',\n",
       " 'cloudy',\n",
       " 'forty',\n",
       " 'collegebound',\n",
       " 'diabetic',\n",
       " 'wherever',\n",
       " 'sheltered',\n",
       " 'grilles',\n",
       " 'bali',\n",
       " 'courtesey',\n",
       " 'foodits',\n",
       " 'yethe',\n",
       " 'college',\n",
       " 'personnally',\n",
       " 'ogonquit',\n",
       " 'tvchef',\n",
       " 'hungrybrowsercom',\n",
       " 'kneehigh',\n",
       " 'okeefe',\n",
       " 'carnicelli',\n",
       " 'dishsoap',\n",
       " 'ships',\n",
       " 'primo',\n",
       " 'sammie',\n",
       " 'kfcs',\n",
       " 'batterwe',\n",
       " 'windhorse',\n",
       " 'build',\n",
       " 'grandmotherinlaw',\n",
       " 'italians',\n",
       " '11am',\n",
       " 'waste',\n",
       " 'texasstyle',\n",
       " 'interior',\n",
       " 'sitsbest',\n",
       " 'anitalove',\n",
       " 'demanded',\n",
       " '2nds',\n",
       " 'yukon',\n",
       " '1901',\n",
       " '52206',\n",
       " 'containerwith',\n",
       " 'spot',\n",
       " 'crockpot',\n",
       " 'moldy',\n",
       " 'nondiabetic',\n",
       " 'pats',\n",
       " 'squeezeable',\n",
       " 'smaller',\n",
       " 'brul',\n",
       " 'seco',\n",
       " 'afterwork',\n",
       " 'accustomed',\n",
       " 'yummyi',\n",
       " 'chickenhot',\n",
       " 'moment',\n",
       " 'medrichs',\n",
       " 'doin',\n",
       " 'chopsnote',\n",
       " 'plaque',\n",
       " 'hurt',\n",
       " 'longlost',\n",
       " 'kirk',\n",
       " 'blesses',\n",
       " 'smells',\n",
       " 'halfbaked',\n",
       " 'exampleshes',\n",
       " 'stopping',\n",
       " 'copious',\n",
       " 'crispyshelled',\n",
       " '50500',\n",
       " 'softer',\n",
       " 'reams',\n",
       " 'elliots',\n",
       " 'pjs',\n",
       " 'whisk',\n",
       " 'bluepurple',\n",
       " 'italianjewish',\n",
       " 'soyoil',\n",
       " 'moderation',\n",
       " 'barbq',\n",
       " 'timemushrooms',\n",
       " 'tryed',\n",
       " 'costcutting',\n",
       " 'maeve',\n",
       " 'arabiandish',\n",
       " 'goods',\n",
       " 'kelloggs',\n",
       " 'sayssalad',\n",
       " 'pate',\n",
       " 'ricedelicious',\n",
       " 'clumping',\n",
       " 'oldtype',\n",
       " 'moulie',\n",
       " 'anatomy',\n",
       " 'shift',\n",
       " 'diaganosed',\n",
       " 'decker',\n",
       " 'saturday',\n",
       " 'guarding',\n",
       " 'wholesometoddlerfoodcom',\n",
       " 'shhhi',\n",
       " 'strings',\n",
       " 'bottom',\n",
       " 'radically',\n",
       " 'supply',\n",
       " 'option',\n",
       " 'cotija',\n",
       " 'baklava',\n",
       " 'mushymost',\n",
       " 'sneek',\n",
       " 'flourrecipe',\n",
       " '382068',\n",
       " 'mound',\n",
       " 'waysas',\n",
       " 'buys',\n",
       " 'sipped',\n",
       " 'devonshire',\n",
       " 'rotary',\n",
       " 'halmonie',\n",
       " 'reasonable',\n",
       " 'lactofermentation',\n",
       " 'englands',\n",
       " 'reallyso',\n",
       " 'try',\n",
       " 'lightened',\n",
       " 'croquettes',\n",
       " 'beaches',\n",
       " 'garliccumin',\n",
       " 'tbs',\n",
       " 'drambuie',\n",
       " 'honorary',\n",
       " 'idiotproof',\n",
       " 'formed',\n",
       " 'norfolk',\n",
       " 'httpwwwugaedunchfphowcanhomehtml',\n",
       " 'fan',\n",
       " 'spahetti',\n",
       " 'smallsize',\n",
       " 'fudges',\n",
       " 'props',\n",
       " 'redbooks',\n",
       " 'go',\n",
       " 'kellerkuchen',\n",
       " 'cindi',\n",
       " 'barr',\n",
       " 'crafting',\n",
       " '200736',\n",
       " 'sickenbergers',\n",
       " 'outif',\n",
       " 'showi',\n",
       " 'creampuffs',\n",
       " 'ozark',\n",
       " 'quinoaor',\n",
       " 'summerserve',\n",
       " 'scallops',\n",
       " 'merry',\n",
       " 'spreadable',\n",
       " 'case',\n",
       " 'schlabberkappes',\n",
       " 'cakeenjoyit',\n",
       " 'houtzer',\n",
       " 'marinara',\n",
       " 'gregg',\n",
       " 'sangue',\n",
       " 'sumant',\n",
       " 'abbott',\n",
       " 'offshore',\n",
       " 'tates',\n",
       " 'mash',\n",
       " 'commercialsized',\n",
       " 'makeyourown',\n",
       " 'complains',\n",
       " 'feel',\n",
       " 'spend',\n",
       " '750',\n",
       " 'baconegg',\n",
       " 'chemo',\n",
       " 'cookbookyou',\n",
       " 'increasing',\n",
       " 'nicolas',\n",
       " 'goe',\n",
       " 'liquor',\n",
       " 'deliciousi',\n",
       " 'vainly',\n",
       " 'ladling',\n",
       " 'bowlsyou',\n",
       " 'lemonade',\n",
       " 'balthazar',\n",
       " 'bloom',\n",
       " 'students',\n",
       " 'sensual',\n",
       " 'noheat',\n",
       " 'ambience',\n",
       " 'schulmans',\n",
       " 'tinkerbell',\n",
       " 'appley',\n",
       " 'pappas',\n",
       " 'brazilnuts',\n",
       " 'blaine',\n",
       " 'glaceed',\n",
       " 'do',\n",
       " 'stokeontrent',\n",
       " '72011',\n",
       " 'upgreat',\n",
       " 'upit',\n",
       " 'definitely',\n",
       " 'awesomevery',\n",
       " 'doz',\n",
       " 'pots',\n",
       " 'zwt7africai',\n",
       " 'silks',\n",
       " 'balti',\n",
       " 'readi',\n",
       " 'shears',\n",
       " 'koosa',\n",
       " 'peshawari',\n",
       " 'collected',\n",
       " '03',\n",
       " 'swee',\n",
       " 'perferably',\n",
       " 'cheddarmozza',\n",
       " 'tone',\n",
       " 'visited',\n",
       " 'des',\n",
       " 'injector',\n",
       " 'tender',\n",
       " 'wellthis',\n",
       " 'dei',\n",
       " 'weave',\n",
       " 'gogh',\n",
       " 'chevys',\n",
       " 'maurer',\n",
       " 'jalepenos',\n",
       " 'mmmwhen',\n",
       " 'marfatia',\n",
       " 'tysons',\n",
       " 'sorry',\n",
       " 'marcias',\n",
       " 'jadot',\n",
       " 'farimount',\n",
       " 'poarch',\n",
       " 'roast',\n",
       " 'nick',\n",
       " 'yogurtyou',\n",
       " 'toenjoy',\n",
       " 'meat',\n",
       " 'poolside',\n",
       " 'pectin',\n",
       " 'prepwork',\n",
       " 'monkfishit',\n",
       " 'tornoff',\n",
       " 'enjoymakes',\n",
       " 'gourmetstyle',\n",
       " 'spinachlover',\n",
       " 'thighsthey',\n",
       " 'poohanpiglet',\n",
       " 'surpass',\n",
       " 'storeetasince',\n",
       " 'motto',\n",
       " 'chunksmild',\n",
       " 'searer',\n",
       " 'races',\n",
       " 'islands',\n",
       " 'creametc',\n",
       " 'initially',\n",
       " 'fantabulous',\n",
       " 'snickerdoodle',\n",
       " 'faith',\n",
       " 'zay',\n",
       " 'cultural',\n",
       " '11th06',\n",
       " 'viva',\n",
       " 'weil',\n",
       " 'avis',\n",
       " 'secondary',\n",
       " '156',\n",
       " 'exploits',\n",
       " 'investment',\n",
       " 'gotland',\n",
       " 'camp',\n",
       " '2010for',\n",
       " 'consulting',\n",
       " 'brewing',\n",
       " 'shoots',\n",
       " 'joggers',\n",
       " 'littleneck',\n",
       " 'fam',\n",
       " 'matza',\n",
       " 'recipenot',\n",
       " 'genericstorebrand',\n",
       " '1963',\n",
       " 'coktails',\n",
       " 'glistening',\n",
       " 'scream',\n",
       " 'httpwwwgreenprophetcom',\n",
       " 'gilroy',\n",
       " '30min',\n",
       " 'stovvetop',\n",
       " 'zesting',\n",
       " 'finalists',\n",
       " 'heartiness',\n",
       " 'swoons',\n",
       " 'grownups',\n",
       " 'callnote',\n",
       " 'mccoys',\n",
       " 'tastyi',\n",
       " 'currently',\n",
       " 'likely',\n",
       " 'dilly',\n",
       " 'tater',\n",
       " 'plopping',\n",
       " 'wayhttpwwwelanaspantrycomcucumberavocadogazpacho',\n",
       " 'lavender',\n",
       " 'bubelach',\n",
       " 'so',\n",
       " 'canape',\n",
       " 'twinkie',\n",
       " 'syndicated',\n",
       " 'countdown',\n",
       " 'labled',\n",
       " 'twistsausages',\n",
       " '10sec',\n",
       " 'magazinedated010204',\n",
       " 'harvey',\n",
       " 'ajinomoto',\n",
       " 'rinsed',\n",
       " 'softness',\n",
       " 'photocopied',\n",
       " 'occasionlunch',\n",
       " 'reconstitue',\n",
       " 'comprises',\n",
       " 'farmhouses',\n",
       " 'notebook',\n",
       " 'lucious',\n",
       " 'heston',\n",
       " 'obliged',\n",
       " 'boot',\n",
       " 'viennayou',\n",
       " 'pasta',\n",
       " 'waukesha',\n",
       " 'matahami',\n",
       " 'adrin',\n",
       " 'elvira',\n",
       " 'sudha',\n",
       " 'occasionnote',\n",
       " 'porchetta',\n",
       " 'fails',\n",
       " 'intriguingto',\n",
       " 'pintos',\n",
       " 'plucked',\n",
       " 'additives',\n",
       " '101',\n",
       " 'cukes',\n",
       " 'kolomo',\n",
       " 'rigate',\n",
       " 'adhere',\n",
       " 'utilizing',\n",
       " 'cars',\n",
       " 'ode',\n",
       " '3inch',\n",
       " 'fitting',\n",
       " 'samoa',\n",
       " 'treat',\n",
       " 'detour',\n",
       " 'piles',\n",
       " 'guarded',\n",
       " 'nutsin',\n",
       " 'partyline',\n",
       " 'ten',\n",
       " 'mhemmer',\n",
       " 'tablei',\n",
       " 'les',\n",
       " 'pantryit',\n",
       " '350f',\n",
       " 'pinching',\n",
       " '206230your',\n",
       " 'httpwwwdoctorozcomrecipebananatea',\n",
       " 'evoking',\n",
       " 'anjou',\n",
       " 'finally',\n",
       " 'stump',\n",
       " 'breadmaker',\n",
       " 'collards',\n",
       " 'multigrain',\n",
       " '42g',\n",
       " 'conquer',\n",
       " 'toronto',\n",
       " 'ricespaetzle',\n",
       " '43g',\n",
       " 'prediabetes',\n",
       " 'singing',\n",
       " 'fastif',\n",
       " 'breadfound',\n",
       " 'refill',\n",
       " '52272',\n",
       " 'hughes',\n",
       " 'fy',\n",
       " 'fishing',\n",
       " 'kathryn',\n",
       " 'nutsseeds',\n",
       " 'knowstrange',\n",
       " 'ry',\n",
       " '1991',\n",
       " '373985',\n",
       " 'pecans',\n",
       " 'creammy',\n",
       " 'orig',\n",
       " 'denver',\n",
       " 'motivated',\n",
       " 'goodcan',\n",
       " 'somewhere',\n",
       " 'detailed',\n",
       " 'cheeky',\n",
       " 'substitutions',\n",
       " 'description',\n",
       " 'puddinggelatin',\n",
       " 'actually',\n",
       " 'deadeasy',\n",
       " 'destroy',\n",
       " 'practicing',\n",
       " 'dion',\n",
       " 'liquidonly',\n",
       " 'loversserve',\n",
       " 'scottsdale',\n",
       " 'seersucker',\n",
       " 'combats',\n",
       " 'cruff',\n",
       " 'tweakable',\n",
       " 'diner524',\n",
       " 'mock',\n",
       " 'inventing',\n",
       " 'featured',\n",
       " 'agnes',\n",
       " 'pereznitza',\n",
       " 'rv',\n",
       " 'intensify',\n",
       " 'yunetthomas',\n",
       " 'schmaltz',\n",
       " 'online',\n",
       " 'birthday',\n",
       " 'noir',\n",
       " 'showstoppingly',\n",
       " 'healthyhttparticlesmercolacomsitesarticlesarchive20021113eggsparttwoaspx',\n",
       " 'butterfly',\n",
       " 'kung',\n",
       " 'herhehe',\n",
       " 'xs',\n",
       " 'archiduc',\n",
       " 'gardenripe',\n",
       " 'fame',\n",
       " '41st',\n",
       " 'noting',\n",
       " 'cakebut',\n",
       " 'pioneers',\n",
       " 'olds',\n",
       " 'sterile',\n",
       " 'perphaps',\n",
       " 'duran',\n",
       " 'alsace',\n",
       " 'guesses',\n",
       " 'handrolled',\n",
       " 'httpwwweversavecomeversaveconsumersrecipejsp',\n",
       " 'witlof',\n",
       " 'elmotoo',\n",
       " '207616s',\n",
       " 'jacob',\n",
       " 'lammes',\n",
       " 'soledad',\n",
       " 'growing',\n",
       " 'impromptu',\n",
       " 'mockpotroast',\n",
       " 'potlucknote',\n",
       " 'fruitpromise',\n",
       " 'andb',\n",
       " 'san',\n",
       " 'corriher',\n",
       " 'thompsons',\n",
       " 'websiteblog',\n",
       " 'wakes',\n",
       " 'downstairs',\n",
       " 'magazinemarch',\n",
       " 'barrio',\n",
       " 'eatingwellcom',\n",
       " 'filler',\n",
       " 'adaption',\n",
       " 'bringing',\n",
       " 'beauty',\n",
       " '616',\n",
       " 'misanthropes',\n",
       " 'wakarusa',\n",
       " 'nyama',\n",
       " 'lookingforwarmer',\n",
       " '418911',\n",
       " 'itmust',\n",
       " 'bahamian',\n",
       " 'functions',\n",
       " 'marks',\n",
       " 'rectangle',\n",
       " 'passovers',\n",
       " 'onehalf',\n",
       " 'advocaat',\n",
       " 'cakethe',\n",
       " 'potatoesthe',\n",
       " 'myself',\n",
       " 'shut',\n",
       " '330',\n",
       " 'tonight',\n",
       " 'recharged',\n",
       " 'regionconch',\n",
       " 'suspendedthe',\n",
       " 'fullesttrust',\n",
       " 'pastosas',\n",
       " 'fewbut',\n",
       " 'flavourings',\n",
       " 'septembers',\n",
       " 'jackies',\n",
       " 'barley',\n",
       " 'opinon',\n",
       " 'soughtafter',\n",
       " 'gained',\n",
       " 'bhunni',\n",
       " '458602',\n",
       " 'tastless',\n",
       " 'feathers',\n",
       " 'holland',\n",
       " 'enquirer',\n",
       " 'bestselling',\n",
       " '6ounce',\n",
       " 'makehope',\n",
       " 'httpwwwrecipezaarcombbviewtopiczspt261947postdays0postorderascstart30',\n",
       " 'nonrecipe',\n",
       " 'genius',\n",
       " 'healthyim',\n",
       " 'stripscooking',\n",
       " 'lucias',\n",
       " 'addiction',\n",
       " 'touching',\n",
       " 'mit',\n",
       " 'nonstop',\n",
       " 'wetter',\n",
       " 'himself',\n",
       " 'hails',\n",
       " 'tucked',\n",
       " 'temp',\n",
       " 'attempts',\n",
       " 'sip',\n",
       " 'el',\n",
       " 'chilijalapeno',\n",
       " 'blind',\n",
       " 'browneyedbaker',\n",
       " 'spices',\n",
       " 'diegoplus',\n",
       " 'decadant',\n",
       " 'showed',\n",
       " 'alongs',\n",
       " 'koval',\n",
       " 'literature',\n",
       " 'timesee',\n",
       " 'lovin',\n",
       " 'pastes',\n",
       " 'ls',\n",
       " 'ribs',\n",
       " 'recipeoriented',\n",
       " 'spoke',\n",
       " 'bonein',\n",
       " 'krishna',\n",
       " 'haunting',\n",
       " 'dietetic',\n",
       " 'certo',\n",
       " 'wal',\n",
       " 'broiled',\n",
       " 'ample',\n",
       " 'belvedere',\n",
       " 'kitchenaidtype',\n",
       " 'discussing',\n",
       " 'spinachartichoke',\n",
       " 'firstpublished',\n",
       " 'set',\n",
       " 'boggled',\n",
       " 'defer',\n",
       " 'pannans',\n",
       " 'chores',\n",
       " 'bucks',\n",
       " 'compromising',\n",
       " 'prounced',\n",
       " 'vegetarain',\n",
       " 'mexicotexmexsw',\n",
       " '41g',\n",
       " 'marinera',\n",
       " 'deliciousps',\n",
       " 'bevcookscom',\n",
       " 'pairingswine',\n",
       " 'specific',\n",
       " 'seeds',\n",
       " 'ordering',\n",
       " 'wellcooking',\n",
       " 'nori',\n",
       " 'delhi',\n",
       " 'unoaked',\n",
       " 'wiki',\n",
       " 'doodle',\n",
       " 'succumb',\n",
       " 'knowif',\n",
       " 'azteca',\n",
       " '141328',\n",
       " 'littlei',\n",
       " 'falafels',\n",
       " 'breadyou',\n",
       " 'monroe',\n",
       " 'incidentally',\n",
       " 'neiman',\n",
       " 'centers',\n",
       " 'wisked',\n",
       " 'kochen',\n",
       " 'chickenchicken',\n",
       " 'magazineissue',\n",
       " 'httpwwwroyersroundtopcafecom',\n",
       " 'clotilde',\n",
       " 'lalanne',\n",
       " 'wursts',\n",
       " 'southwesternmexican',\n",
       " 'scarfed',\n",
       " 'eyes',\n",
       " 'blenders',\n",
       " 'morebaked',\n",
       " 'fishers',\n",
       " 'photocopy',\n",
       " 'torte',\n",
       " 'carne',\n",
       " 'selftitled',\n",
       " '145891',\n",
       " 'strawberrymy',\n",
       " 'surfing',\n",
       " 'fattysugar',\n",
       " 'darlin',\n",
       " 'churros',\n",
       " 'jump',\n",
       " 'frostings',\n",
       " 'sacred',\n",
       " 'author',\n",
       " 'driveins',\n",
       " 'them',\n",
       " 'dissolve',\n",
       " 'cutters',\n",
       " 'fullycooked',\n",
       " 'carmen',\n",
       " 'juan',\n",
       " 'th',\n",
       " 'massaging',\n",
       " 'comfortingoh',\n",
       " 'seasonthe',\n",
       " 'soulful',\n",
       " 'kiesel',\n",
       " 'zaarite',\n",
       " 'daylight',\n",
       " 'sameexperiment',\n",
       " 'novice',\n",
       " 'rehab',\n",
       " 'rooibos',\n",
       " 'yoshinoya',\n",
       " 'chaya',\n",
       " 'garbanzos',\n",
       " 'tasmanian',\n",
       " 'screaming',\n",
       " 'calorieno',\n",
       " 'beefycheesy',\n",
       " 'custardthe',\n",
       " 'httpwww24hourmomcom',\n",
       " 'maven',\n",
       " 'yeah',\n",
       " 'wis',\n",
       " 'grate',\n",
       " 'activate',\n",
       " 'letmetellyou',\n",
       " 'guerrero',\n",
       " 'gloria',\n",
       " 'slimfast',\n",
       " 'beleive',\n",
       " 'earl',\n",
       " 'moly',\n",
       " 'ceecee526',\n",
       " 'rivals',\n",
       " 'incorporate',\n",
       " 'splendahalf',\n",
       " '200gram',\n",
       " 'httpwwwbbcgoodfoodcomrecipes9099lancashirehotpot',\n",
       " 'lunchupdatei',\n",
       " 'sum',\n",
       " 'cucumberwhatever',\n",
       " 'gastric',\n",
       " 'alfredotype',\n",
       " 'drumstick',\n",
       " 'enewsletter',\n",
       " 'customtailored',\n",
       " 'nipomo',\n",
       " 'wwwtarladalalcom',\n",
       " 'carville',\n",
       " 'whimsical',\n",
       " 'barbques',\n",
       " 'creatively',\n",
       " 'rehydrating',\n",
       " 'padua',\n",
       " 'warmers',\n",
       " 'consideration',\n",
       " 'hosting',\n",
       " 'nicethis',\n",
       " 'acrossin',\n",
       " 'dick',\n",
       " 'dehydrated',\n",
       " 'dollys',\n",
       " 'nanners',\n",
       " '50s',\n",
       " 'callow',\n",
       " 'pea',\n",
       " 'stirins',\n",
       " 'apportioned',\n",
       " 'cowhorns',\n",
       " 'chipssome',\n",
       " 'fudgecicle',\n",
       " 'breakfastsubmitted',\n",
       " 'nl',\n",
       " 'issur',\n",
       " 'cried',\n",
       " 'row',\n",
       " 'rice',\n",
       " 'crockpotif',\n",
       " 'thoughtrust',\n",
       " 'lowcarbing',\n",
       " 'associations',\n",
       " 'spouse',\n",
       " 'melting',\n",
       " 'dinnertime',\n",
       " 'theseanother',\n",
       " 'oftenservings',\n",
       " 'wwwbreadtopiacomsourdoughwafflesandpancakes',\n",
       " 'randy',\n",
       " 'hulstone',\n",
       " 'mosh',\n",
       " 'guesstimated',\n",
       " 'madewith',\n",
       " 'seasoningthere',\n",
       " 'sucked',\n",
       " 'fingerling',\n",
       " 'ultrabounty',\n",
       " 'oatmealblueberry',\n",
       " 'jorge',\n",
       " 'ay',\n",
       " 'hershey',\n",
       " 'steams',\n",
       " 'handmethod',\n",
       " 'corney',\n",
       " 'recipesthe',\n",
       " 'fuhrmans',\n",
       " 'vichy',\n",
       " 'auntie',\n",
       " 'sundried',\n",
       " 'tooi',\n",
       " 'scatter',\n",
       " 'chichis',\n",
       " 'besttraditionally',\n",
       " 'husbandandwife',\n",
       " 'vote',\n",
       " 'melseth',\n",
       " 'spoils',\n",
       " 'definate',\n",
       " 'saidlittle',\n",
       " 'garliconion',\n",
       " 'pups',\n",
       " 'tb',\n",
       " 'merge',\n",
       " 'nausea',\n",
       " 'zucchini',\n",
       " 'indigo',\n",
       " 'yamunda',\n",
       " 'swede',\n",
       " 'vafound',\n",
       " '4hmy',\n",
       " 'tweaked',\n",
       " 'quasichickenrollups',\n",
       " 'recipebe',\n",
       " 'useroot',\n",
       " 'honorable',\n",
       " 'fired',\n",
       " 'raichlens',\n",
       " 'preventioncom',\n",
       " 'shortbread',\n",
       " 'pluseach',\n",
       " 'shiner',\n",
       " 'carefulthey',\n",
       " 'nonlow',\n",
       " 'separating',\n",
       " 'champagne',\n",
       " '116849',\n",
       " 'offseason',\n",
       " 'ices',\n",
       " 'backwhich',\n",
       " 'changesto',\n",
       " 'sheettype',\n",
       " 'nitty',\n",
       " 'beignet',\n",
       " 'familypleaser',\n",
       " 'joint',\n",
       " 'immensely',\n",
       " '6inch',\n",
       " 'howeverthis',\n",
       " 'eatschecked',\n",
       " 'seperate',\n",
       " 'marinades',\n",
       " 'crisscross',\n",
       " 'hop',\n",
       " 'karmel',\n",
       " 'sealameal',\n",
       " 'ishare',\n",
       " 'devision',\n",
       " 'hovis',\n",
       " 'greenwise',\n",
       " 'href',\n",
       " 'palatethis',\n",
       " 'committee',\n",
       " 'lifting',\n",
       " 'cookbookthe',\n",
       " 'macerate',\n",
       " 'desire',\n",
       " 'thoughbut',\n",
       " 'butter',\n",
       " 'bathers',\n",
       " 'zeasty',\n",
       " 'wellas',\n",
       " 'mmmmmm',\n",
       " 'garliclovers',\n",
       " 'burke',\n",
       " 'qimiaa',\n",
       " 'flourbased',\n",
       " 'feast',\n",
       " 'maribeth',\n",
       " 'stations',\n",
       " 'clovesoften',\n",
       " 'powered',\n",
       " 'naschmarkt',\n",
       " 'thiis',\n",
       " 'juicefor',\n",
       " '33',\n",
       " 'itgracias',\n",
       " 'nice',\n",
       " 'part',\n",
       " 'quichelike',\n",
       " 'noboil',\n",
       " 'steeping',\n",
       " 'fooddownunder',\n",
       " 'currymania',\n",
       " 'contradiction',\n",
       " 'kobayashi',\n",
       " 'polished',\n",
       " 'sented',\n",
       " 'vedic',\n",
       " 'zero',\n",
       " 'indonesian',\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = set()\n",
    "for i in df['preprocessed_descriptions'].values:\n",
    "    words.update(word_tokenize(str(i).lower()))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. gravythis     & decorating : расстояние  9\n",
      "2. christmasyou  & honeys     : расстояние  10\n",
      "3. midweek       & comprises  : расстояние  7\n",
      "4. streamline    & reheats    : расстояние  8\n",
      "5. thanksgivings & distributed: расстояние  12\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "import random\n",
    "word1 = random.choices(list(words), k=5)\n",
    "word2 = random.choices(list(words - set(word1)), k=5)\n",
    "pair = zip(word1, word2) \n",
    "for index, (k, v) in enumerate(pair):\n",
    "    print(f\"{index+1}. {k:<{max(map(len, word1))}} & {v:<{max(map(len, word2))}}: расстояние  {edit_distance(k, v)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 0), ('jello', 1), ('hell', 1), ('mello', 1), ('fillo', 2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_closest(word, k):\n",
    "    wlist = list(words)\n",
    "    wsorted = sorted(enumerate(map(lambda x: edit_distance(word, x), wlist)), key = lambda x: x[1])\n",
    "    res = list(map(lambda x: (wlist[x[0]],x[1]), wsorted[:k]))\n",
    "    return res\n",
    "\n",
    "get_closest('hello', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sysoevkd/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sysoevkd/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>equally</th>\n",
       "      <td>equal</td>\n",
       "      <td>equally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreaming</th>\n",
       "      <td>dream</td>\n",
       "      <td>dreaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factor</th>\n",
       "      <td>factor</td>\n",
       "      <td>factor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adviehbaharat</th>\n",
       "      <td>adviehbaharat</td>\n",
       "      <td>adviehbaharat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>otherwisebam</th>\n",
       "      <td>otherwisebam</td>\n",
       "      <td>otherwisebam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vess</th>\n",
       "      <td>vess</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pamelas</th>\n",
       "      <td>pamela</td>\n",
       "      <td>pamelas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japalenos</th>\n",
       "      <td>japaleno</td>\n",
       "      <td>japalenos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nutritioushigh</th>\n",
       "      <td>nutritioushigh</td>\n",
       "      <td>nutritioushigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>croque</th>\n",
       "      <td>croqu</td>\n",
       "      <td>croque</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  stemmed_word normalized_word\n",
       "equally                  equal         equally\n",
       "dreaming                 dream        dreaming\n",
       "factor                  factor          factor\n",
       "adviehbaharat    adviehbaharat   adviehbaharat\n",
       "otherwisebam      otherwisebam    otherwisebam\n",
       "...                        ...             ...\n",
       "vess                      vess               f\n",
       "pamelas                 pamela         pamelas\n",
       "japalenos             japaleno       japalenos\n",
       "nutritioushigh  nutritioushigh  nutritioushigh\n",
       "croque                   croqu          croque\n",
       "\n",
       "[32868 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stem = SnowballStemmer('english')\n",
    "wlist = list(words)\n",
    "words_df = pd.DataFrame(dict(stemmed_word=map(stem.stem, wlist), normalized_word=map(wnl.lemmatize, wlist)), index = wlist)\n",
    "words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['edit_distance'] = words_df.apply(lambda x: edit_distance(x.stemmed_word, x.normalized_word), axis=1)\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sysoevkd/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069885"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for i in df.preprocessed_descriptions:\n",
    "    counter.update(word_tokenize(str(i).lower()))\n",
    "n_words_old = sum(counter.values()); n_words_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 40072),\n",
       " ('a', 34951),\n",
       " ('and', 30245),\n",
       " ('this', 26859),\n",
       " ('i', 24836),\n",
       " ('to', 23471),\n",
       " ('is', 20285),\n",
       " ('it', 19756),\n",
       " ('of', 18364),\n",
       " ('for', 15939)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп слова составляли 54.4% от общего числа слов\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "keys = list(counter.keys())\n",
    "for i in keys:\n",
    "    if i in en_stopwords:\n",
    "        counter.pop(i)\n",
    "print(f'Стоп слова составляли {round(sum(counter.values()) / n_words_old * 100, 1)}% от общего числа слов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recipe', 14871),\n",
       " ('make', 6326),\n",
       " ('time', 5137),\n",
       " ('use', 4620),\n",
       " ('great', 4430),\n",
       " ('like', 4167),\n",
       " ('easy', 4152),\n",
       " ('one', 3872),\n",
       " ('made', 3810),\n",
       " ('good', 3791)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21622178, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.21622178, 0.        , 0.17444637, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34889275, 0.21622178, 0.        ,\n",
       "       0.        , 0.21622178, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24363116, 0.14480625, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.21622178, 0.21622178, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.21622178, 0.        , 0.        ,\n",
       "       0.        , 0.21622178, 0.21622178, 0.21622178, 0.        ,\n",
       "       0.        , 0.14480625, 0.21622178, 0.        , 0.        ,\n",
       "       0.        , 0.21622178, 0.21622178, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17444637,\n",
       "       0.21622178, 0.        , 0.12181558, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.21622178,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sample = df.sample(5)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sample.preprocessed_descriptions.values).toarray()\n",
    "vectors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delicious coconut custard pie</th>\n",
       "      <th>moms peanut butter roll</th>\n",
       "      <th>crustless brownie pie  from my great recipes</th>\n",
       "      <th>smoked salmon scramble</th>\n",
       "      <th>martha stewart s honey mustard sauce</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delicious coconut custard pie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.931272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moms peanut butter roll</th>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.902202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.904346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crustless brownie pie  from my great recipes</th>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.902202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoked salmon scramble</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>martha stewart s honey mustard sauce</th>\n",
       "      <td>0.931272</td>\n",
       "      <td>0.904346</td>\n",
       "      <td>0.875123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              delicious coconut custard pie  \\\n",
       "name                                                                          \n",
       "delicious coconut custard pie                                      0.000000   \n",
       "moms peanut butter roll                                            0.876691   \n",
       "crustless brownie pie  from my great recipes                       0.861100   \n",
       "smoked salmon scramble                                             1.000000   \n",
       "martha stewart s honey mustard sauce                               0.931272   \n",
       "\n",
       "                                              moms peanut butter roll  \\\n",
       "name                                                                    \n",
       "delicious coconut custard pie                                0.876691   \n",
       "moms peanut butter roll                                      0.000000   \n",
       "crustless brownie pie  from my great recipes                 0.902202   \n",
       "smoked salmon scramble                                       1.000000   \n",
       "martha stewart s honey mustard sauce                         0.904346   \n",
       "\n",
       "                                              crustless brownie pie  from my great recipes  \\\n",
       "name                                                                                         \n",
       "delicious coconut custard pie                                                     0.861100   \n",
       "moms peanut butter roll                                                           0.902202   \n",
       "crustless brownie pie  from my great recipes                                      0.000000   \n",
       "smoked salmon scramble                                                            1.000000   \n",
       "martha stewart s honey mustard sauce                                              0.875123   \n",
       "\n",
       "                                              smoked salmon scramble  \\\n",
       "name                                                                   \n",
       "delicious coconut custard pie                                    1.0   \n",
       "moms peanut butter roll                                          1.0   \n",
       "crustless brownie pie  from my great recipes                     1.0   \n",
       "smoked salmon scramble                                           0.0   \n",
       "martha stewart s honey mustard sauce                             1.0   \n",
       "\n",
       "                                              martha stewart s honey mustard sauce  \n",
       "name                                                                                \n",
       "delicious coconut custard pie                                             0.931272  \n",
       "moms peanut butter roll                                                   0.904346  \n",
       "crustless brownie pie  from my great recipes                              0.875123  \n",
       "smoked salmon scramble                                                    1.000000  \n",
       "martha stewart s honey mustard sauce                                      0.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "matrix = pd.DataFrame({i:[cosine(vectors[index], vectors[index2]) for index2, k in enumerate(sample.name)] for index,i in enumerate(sample.name)}, index=sample.name)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее похожие рецепты: \n",
      "1)delicious coconut custard pie\n",
      "2)smoked salmon scramble\n",
      " Расстрояние между ними равно: 1.0\n"
     ]
    }
   ],
   "source": [
    "row_max, row_index = matrix.values.max(axis=1), matrix.values.argmax(axis=1)\n",
    "col_max, col_index = max(row_max), np.argmax(row_max)\n",
    "\n",
    "print(f'Наиболее похожие рецепты: \\n1){matrix.columns[col_index]}\\n2){matrix.index.values[row_index][col_index]}\\n Расстрояние между ними равно: {col_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
